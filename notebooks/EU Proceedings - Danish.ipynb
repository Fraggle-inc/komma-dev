{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EU Proceedings - Danish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import regex\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import regularizers\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, LSTM, Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import komma_dev.eu\n",
    "from komma_dev.vocabulary import build_vocabulary\n",
    "from komma_dev.parsing import StringParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: europarl-v7.da-en.da\n"
     ]
    }
   ],
   "source": [
    "eu_data_raw = komma_dev.eu.load('da')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove leading and trailing whitespace. When running on real user input this should be done separately and the amount and kind of removed whitespace should be stored and eventually added back onto the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eu_data_raw = [line.strip() for line in eu_data_raw]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now parse each string using our StringParser class. This may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = StringParser()\n",
    "eu_data = [parser.parse(line) for line in eu_data_raw]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split\n",
    "The very first thing we need to do after having loaded the data is to create a train/dev split. We will eventually build a vocabulary, and it's important that we build this on the training data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eu_data_train, eu_data_dev = train_test_split(eu_data, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1870360"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eu_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98440"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eu_data_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary\n",
    "We can now build the vocabulary on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab, _ = build_vocabulary(eu_data_train, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_CHUNKS = 50\n",
    "\n",
    "def embed_sentence(sentence, vocabulary):\n",
    "    embedded = [0] * MAX_CHUNKSMAX_TOKENS\n",
    "    for index, chunk in enumerate(sentence.chunks[:MAX_CHUNKSMAX_TOKENS]):\n",
    "        word = chunk.clean_name\n",
    "        if word in vocabulary:\n",
    "            word_id = vocabulary[word]\n",
    "            embedded[index] = word_id\n",
    "        else:\n",
    "            # TODO: don't encode unknown as zero (probably use max(IDs) + 1)\n",
    "            embedded[index] = 0\n",
    "    return embedded\n",
    "\n",
    "def pre_process(sentences, vocabulary):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for sentence in sentences:\n",
    "        # TODO: Remove punctuation except comma?\n",
    "        # sentence = remove_punctuation(sentence)\n",
    "        y = sentence.commas[:50] + ([0] * (MAX_CHUNKS-len(sentence.commas)))\n",
    "        y = np.array(y)[np.newaxis]\n",
    "        embedded_sentence = embed_sentence(sentence, vocabulary)\n",
    "        X.append(embedded_sentence)\n",
    "        Y.append(y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = pre_process(eu_data_train, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1870360\n",
      "1870360\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dev, Y_dev = pre_process(eu_data_dev, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98440\n",
      "98440\n"
     ]
    }
   ],
   "source": [
    "print(len(X_dev))\n",
    "print(len(Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: This only called to get NP arrays, so maybe make sure the X and Y are already NP arrays.\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=50)\n",
    "Y_train = np.concatenate(Y_train, axis=0)\n",
    "\n",
    "X_dev = sequence.pad_sequences(X_dev, maxlen=50)\n",
    "Y_dev = np.concatenate(Y_dev, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1870360, 50)\n",
      "Y_train shape: (1870360, 50)\n",
      "X_dev shape: (98440, 50)\n",
      "Y_dev shape: (98440, 50)\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print('X_dev shape:', X_dev.shape)\n",
    "print('Y_dev shape:', Y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(vocab) + 1, 128, input_length=50))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(filters=64,\n",
    "                 kernel_size=5,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 128)           6400128   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 46, 64)            41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 11, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                23000     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 50)                0         \n",
      "=================================================================\n",
      "Total params: 6,466,702\n",
      "Trainable params: 6,466,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1870360 samples, validate on 98440 samples\n",
      "Epoch 1/5\n",
      "1870360/1870360 [==============================] - 2415s - loss: 0.0724 - acc: 0.9772 - val_loss: 0.0428 - val_acc: 0.9871\n",
      "Epoch 2/5\n",
      "1870360/1870360 [==============================] - 2576s - loss: 0.0386 - acc: 0.9885 - val_loss: 0.0321 - val_acc: 0.9907\n",
      "Epoch 3/5\n",
      "1870360/1870360 [==============================] - 2582s - loss: 0.0320 - acc: 0.9906 - val_loss: 0.0283 - val_acc: 0.9918\n",
      "Epoch 4/5\n",
      "1870360/1870360 [==============================] - 2457s - loss: 0.0293 - acc: 0.9913 - val_loss: 0.0268 - val_acc: 0.9922\n",
      "Epoch 5/5\n",
      "1870360/1870360 [==============================] - 2573s - loss: 0.0279 - acc: 0.9917 - val_loss: 0.0260 - val_acc: 0.9924\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          verbose=1,\n",
    "          validation_data=(X_dev, Y_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_commas(sentence, commas):\n",
    "    assert len(sentence.chunks) == len(commas)\n",
    "    temp = \"\"\n",
    "    for chunk, comma in zip(sentence.chunks, commas):\n",
    "        if comma == chunk.comma:\n",
    "            temp += chunk.name + chunk.trailing_whitespace\n",
    "            continue\n",
    "        \n",
    "        if comma:\n",
    "            temp += chunk.name + \",\" + chunk.trailing_whitespace\n",
    "        else:\n",
    "            temp += chunk.name[:-1] + chunk.trailing_whitespace\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "original: Ja det var hyggeligt.\n",
      "fixed: Ja, det var hyggeligt.\n"
     ]
    }
   ],
   "source": [
    "#example = 'Dette er en sætning som der burde indholde et komma .'\n",
    "#example = \"\"\"det bliver bedre hvis du øver dig lidt først\"\"\"\n",
    "#example = \"jeg spiser kage hvad spiser du?\"\n",
    "#example = \"ved du hvad tid toget kører?\"\n",
    "#example = str(eu_data_dev[1])\n",
    "#example = \"Og til gengæld sker der det og jeg gentager at den samlede landbrugsstøtte vil stige med hele 8% indtil 2013 uden at medregne optagelsen af Bulgarien og Rumænien der er planlagt til 2007.\"\n",
    "example = \"Ja det var hyggeligt.\"\n",
    "example = parser.parse(example)\n",
    "embedded = embed_sentence(example, vocab)\n",
    "embedded = sequence.pad_sequences([embedded], maxlen=50)\n",
    "yhat = model.predict_proba(embedded)\n",
    "y_hat_commas = yhat[0][:len(example.chunks)] >= 0.5\n",
    "print(\"original:\", example)\n",
    "print(\"fixed:\", apply_commas(example, y_hat_commas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
